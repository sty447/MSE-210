---
title: "Lab 10"
author: "Stanley Yang"
date: "`r Sys.Date()`"
output: openintro::lab_report
---

```{r load-packages, message=FALSE}
library(tidyverse)
library(openintro)
library(statsr)
library(broom)
```





## Special notice 

The value of upper bound and lower bound of exercise 2 has been lower from 1-10V to 0.5-2.5V.

The value of upper bound and lower bound of exercise 3 has been lower from 12.5-30V to 2.5-7V.

Cause of change:When the voltage is higher than 7V the thermistor is overloaded by the current.

These changes were made in accord to Dr. Krishna instruction.

voltage is in v

current is in mA


### Exercise 2.1

Q:What are the dimensions of the dataset? What does each row represent?

A: 2 columns x 32 rows, each row represent the output current caused by the input voltage 


```{r code-chunk-label}

low_v = read_csv("data/low_v.csv", show_col_types = FALSE)
high_v = read_csv("data/high_v.csv", show_col_types = FALSE)


all_v = bind_rows(low_v,high_v)
glimpse(all_v)

```

### Exercise 2.2

Q:We can visualize parts of the data (for instance V <= 5 )

Notice: since the boundary were changed, v is set 1.5 

```{r}

low_v5 <- all_v %>% 
  filter( V <= 1.5)

## v5 in this case is 1.5 or lower

low_v5
```



### Exercise 2.3

Q;What type of plot would you use to display the relationship between the voltage as a function of current ?

A:line plot

```{r}

ggplot(low_v5) + geom_line(aes(x=I,y=V))


```



### Exercise 2.4

Q;display the relationship between the voltage as a function of current. For this you need to calculate the resistance by mutating the data.

A: The resistant is constant, this is shown on the graph.

```{r}

low_v5 = low_v5 %>% mutate(R=V/I)
ggplot(low_v5) + geom_line(aes(x=I,y=R)) + ylim(0,2.5)


```

### Exercise 2.5

Q;If the relationship looks linear, we can quantify the strength of the relationship with the correlation coefficient.

A: The correlation coefficient is 0.9993129	which is really close to 1 signifying strong relationship.

```{r}

low_v5 %>%
  summarise(cor(I, V))


```

### Exercise 2.6

Q;Looking at your plot from the previous exercise, describe the relationship between these two variables. Make sure to discuss the form, direction, and strength of the relationship as well as any unusual observations.


A: As seen on the graph the form is a linear equation of y = mx+b,  direction is go toward positive infinity x and y, the relationship strong


```{r}
plot_ss(x = I, y = V, data = low_v5)
```

### Exercise 2.7

Q;Using plot_ss, choose a line that does a good job of minimizing the sum of squares. Run the function several times. What was the smallest sum of squares that you got? How does it compare to your neighbours?


A; The smallest Sum of Squares is 0.002, the neighbor's data in comparison to will be similar.

Notices png of the graph is shown by generating the code in exercise 6 


```{r}


```


### Exercise 3.1

Q;However the original model does (may) not make physical sense since the voltage at zero current is not zero. To force this physical constraint, we need to force the intercept to zero by replacing the formula y ~ x with y ~ 0+x.


```{r}

m1 <- lm(V ~ I, data = low_v5)
tidy(m1)
glance(m1)

m2 <- lm(V ~ 0+I, data = low_v5)
tidy(m2)
glance(m2)

```


### Exercise 3.2

Q;If someone saw the least squares regression line and not the actual data, how would they predict a Voltage for I not in your data ? Is it valid to use model to predict Voltage for 5mA ?

A: If someone saw the least squares regression line and not the actual data, they can predict a Voltage for I not in your data by insert the I value into least squares regression line equation, It isn't valid to use the model to predict the voltage for 5mA as the value deviate too far from the data.


```{r}

ggplot(data = low_v5, aes(x = I, y = V)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ 0+x, se = FALSE)

```


### Exercise 3.3

Q;Is there any apparent pattern in the residuals plot? What does this indicate about the linearity of the relationship between the two variables?

A: yes, there is any apparent pattern, it's even disturbed on both side of the residuals line, this indicts that it's linear relationship between the two variable.


```{r}

m1_aug <- augment(m1)

ggplot(data = m1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  xlab("Fitted values") +
  ylab("Residuals")

```

### Exercise 3.4

Q;Based on the histogram, does the nearly normal residuals condition appear to be violated? Why or why not?

A:the nearly normal residuals condition appear to be violated, as shown by the graph being not normally distributed; however, this is expected as the sample size is less than 30.


```{r}


ggplot(data = m1_aug, aes(x = .resid)) +
  geom_histogram(binwidth = 0.005) +
  xlab("Residuals")


```


### Exercise 3.5

Q;Based on the residuals vs. fitted plot, does the constant variability condition appear to be violated? Why or why not?

A:Based on the residuals vs fitted plot, the constant variability conditions appear to be not violated as it's shown there is constant variance with exception of the 2 outliers.

```{r}


```


### More Practice 1

Q:How well do you think R will be correlated with I and V ? Produce a scatterplot of the R vs I and R vs V variables and fit a linear model. At a glance, does there seem to be a linear relationship?

A:  It seems like it correlates fairly well between I and V, but it seemed more correlates towards R and I. At a glance looks to be in a linear relationship.

```{r}

ggplot(data = low_v5, aes(x = R, y = I)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) 

ggplot(data = low_v5, aes(x = R, y = V)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE)

```


### More Practice 2

Q:How does this relationship compare to the relationship between I and V? Use the R2 values from the two model summaries to compare. Which of your independent variable (ie I or V) seem to predict R better? Why or why not?

A: the relationship looks to be the same,  current seem to predict R better because V = IR and that V/I = R.


### More Practice 3

Q: Check the model diagnostics using appropriate visualizations and evaluate if the model conditions have been met.

A: Checked below condition 

-Linearity (linear of y vs x),
-Independence (one point is not related to next),
-Normality (normally distributed),
-equal variance (it is not shaped like a cone)




### Exercise 4.1

Q:What type of plot would you use to display the relationship between the voltage as a function of current ?

A: Line plot is use to display the relationship between the voltage as a function of current, as it clearly show the relationship



```{r}

ggplot(high_v) + geom_line(aes(x=I,y=V))

```

### Exercise 4.2

Q:Display the relationship between the voltage as a function of current. For this you need to calculate the resistance by mutating the data


A: looking at the graph the resistance decrease as the current increase.


```{r}

high_v = high_v %>% mutate(R=V/I)
ggplot(high_v) + geom_line(aes(x=I,y=R)) + ylim(0,4)

```



### Exercise 4.3

Q:Create and compare the quadratic model created using different techniques

A: The two quadratic models are fairly similar.In the quadratic fit it look like there are errors that are causing the resistance to go down significantly at higher currents.The R value with 14 degrees to freedom is 0.8564 to 0.9991. From summaries the linear quadratic fit looks like it's better out of the two quadratic models.

```{r}

high_v <- high_v %>%
    mutate(Isq = I^2)
mod2l <- lm(V ~ I, data = high_v)
mod2q_a <- lm(V ~ I + Isq, data = high_v)
mod2q_b <- lm(V ~ I + I(I^2), data = high_v)
mod2q_c <- lm(V ~ poly(I, 2), data = high_v)

summary(mod2l)

summary(mod2q_a)

summary(mod2q_b)

summary(mod2q_c)

mod2l_aug = augment(mod2l,high_v)
mod2q = mod2q_c
mod2q_aug = augment(mod2q_c,high_v)
plt = ggplot(mod2l_aug,aes(x=I,y=V)) +
  geom_point()+
  geom_line(aes(x = I, y = .fitted), col = "blue") +
  geom_line(data =mod2q_aug, aes(x = I, y = .fitted), col = "red")
plt+labs(title = "Linear (blue) vs. Cubic (red) Polynomial Fits")

mod2q0 <- lm(V ~ 0+poly(I, 2), data = high_v)
summary(mod2q0)

mod2q0_aug = augment(mod2q0,high_v)
plt+geom_line(data =mod2q0_aug, aes(x = I, y = .fitted), col = "red", linetype ='3313')
```

### Exercise 4.4

Q:Q: Redo the above command by reducing the degree of the polynomial.

A: It looks like this reduces our R value and gives a high p value compared to before. by using all_v instead of high_v a different result is obtained , with R squared being 0.2552 it seem that the strength of the x and y variables inst that strong.


```{r}

mod2q_c <- lm(V ~ 0+poly(I, 3), data = high_v)
summary(mod2q_c)

mod2q_c <- lm(V ~ 0+poly(I, 3), data = all_v)
summary(mod2q_c)

```
...




